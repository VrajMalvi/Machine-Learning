{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4f4JG1gdKqj"
   },
   "source": [
    "#Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jbiqOK7dLGG"
   },
   "source": [
    "##Downloading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL5MEkLcfRD2"
   },
   "source": [
    "###ML-100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rjOPzue7FCXJ"
   },
   "outputs": [],
   "source": [
    "# !wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "# !unzip ml-100k.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Xis6ldDfTs6"
   },
   "source": [
    "###ML-1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LOly1yfAfTjd"
   },
   "outputs": [],
   "source": [
    "# !wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "# !unzip ml-1m.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOBJ8UCXdY0g"
   },
   "source": [
    "##Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yio7VSb9WdVU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM04FyMudkoK"
   },
   "source": [
    "## Importing the dataset\n",
    "\n",
    "\n",
    "* sep='::' specifies the separator used to separate values in the file. In this case, it's a double colon '::'. This separator is used because the dataset may not be a standard CSV (Comma-Separated Values) file.\n",
    "\n",
    "* header=None indicates that there is no header row in the dataset, and the first row should be treated as data, not column names.\n",
    "\n",
    "* engine='python' specifies the parsing engine to use. In this case, it's set to 'python', which is used when non-standard separators like '::' are used.\n",
    "\n",
    "* encoding='latin-1' specifies the character encoding used in the file.\n",
    "'latin-1' is a commonly used encoding for handling text data. (in our case we used latin-1 as data contains special characters or symbols that are better represented in 'latin-1', using this encoding ensures that those characters are correctly read.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ebt29eMtWdk7"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header=None, engine='python', encoding='latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep = '::', header=None, engine='python', encoding='latin-1')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header=None, engine='python', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTIbE2tkdkwP"
   },
   "source": [
    "## Preparing the training set and the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1u02e2RzWd2n"
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', sep='\\t')\n",
    "# 1 colom - users, 2nd colom - movies and 3rd colom - ratings we dont need 4th colom same in test_set\n",
    "training_set = np.array(training_set, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Oa7vYCqXWeIo"
   },
   "outputs": [],
   "source": [
    "# in test set we have same users but movies associated with the same user is difffernt from training set\n",
    "test_set = pd.read_csv('ml-100k/u1.test', sep='\\t')\n",
    "test_set = np.array(test_set, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCf8HjSydk4s"
   },
   "source": [
    "## Getting the number of users and movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QZeERw9Tmme_"
   },
   "outputs": [],
   "source": [
    "# in upcoming setps we will convert training and test set in to matrix where\n",
    "# line represents - users, colorings - movies and cells - ratings\n",
    "# in each of this two matices we will include all users and movies from original dataset\n",
    "\n",
    "# in cells where user did not rate we will put '0' by default\n",
    "# we want to get maximum number of users in data set if we get max form training set it wont work\n",
    "# as users and movies are not distributeed in same way, it possible that user with highest userID could be in test set\n",
    "# splits are random so we could have highest userID in one of the test set\n",
    "# we will take max of user_id in training and test sets by doing that we will get max number of user and movies\n",
    "\n",
    "# we need to do this step if we want to apply 5 k-cross validation sets from u1 to u5\n",
    "\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))  # here we are taking max of both for the user colunm which is 0th colunm same for movies\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CQIqi3EOrUWc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of users: 943\n",
      "max numebr of movies: 1682\n"
     ]
    }
   ],
   "source": [
    "print('max number of users:', nb_users)\n",
    "print('max numebr of movies:', nb_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-w4-hVidlAm"
   },
   "source": [
    "## Converting the data into an array with users in lines and movies in columns\n",
    "\n",
    "* [data[:, 0] == id_user] creates a boolean mask by comparing the values in the first column (index 0) of the data array to a specific value id_user. This mask will have True where the condition is met and False where it's not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rJpwpQjqWeuW"
   },
   "outputs": [],
   "source": [
    "# we will create matrice which has max user count as row and max movie count and column\n",
    "\n",
    "def convert(data):\n",
    "  # we are creating list of list each line representing unique user\n",
    "  new_data =[]\n",
    "\n",
    "  # user value start form 1 to value of nb_user and in range it does not inclue upperbound so we have to add +1\n",
    "  for id_user in range(1, nb_users + 1):\n",
    "    id_movies = data[:, 1][data[:, 0] == id_user] # contains index/id of movie rated by each user\n",
    "    id_ratings = data[:, 2][data[:, 0] == id_user] # contains ratings rated by each user we will put '0' for movies which are not rated\n",
    "\n",
    "    ratings = np.zeros(nb_movies) # creating rating var containing list of 0, len of total movies\n",
    "\n",
    "    # index of movies start at 1 but index of rating start at zero so we need to substract by 1\n",
    "    ratings[id_movies - 1] = id_ratings # now we have list of rating for all the movies ids for all users\n",
    "\n",
    "    new_data.append(list(ratings)) # added rating to new_data userwise we are creating list of list and rating is array so we need to convert\n",
    "\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eur6x4S0x9dU"
   },
   "outputs": [],
   "source": [
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)\n",
    "# so now we have matrices comntaing ratings for all movies for all user( row-user, column each movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMmhuUpldlHo"
   },
   "source": [
    "## Converting the data into Torch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m-mzd-VsWe-L"
   },
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIPruubGdlPW"
   },
   "source": [
    "## Converting the ratings into binary ratings 1 (Liked) or 0 (Not Liked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RMTPNmciWfbN"
   },
   "outputs": [],
   "source": [
    "# we need to convert rating value to 0's and 1's as RBM will predict input values which are not-rated(0) to liked(1) and not liked(0)\n",
    "# since we are predicting 0 and 1, not-rated movies should have different value like (-1)\n",
    "training_set[training_set == 0] = -1\n",
    "\n",
    "# we can consider that if user has rated movie 1 or 2 out of 5 then he did not like the movie so are changing it to 0(not liked)\n",
    "training_set[training_set == 1] = 0\n",
    "training_set[training_set == 2] = 0\n",
    "\n",
    "# for movie 3 or more rating we are considering user liked movie giving it value 1\n",
    "training_set[training_set >= 3] = 1\n",
    "\n",
    "\n",
    "# same for test set\n",
    "test_set[test_set == 0] = -1\n",
    "test_set[test_set == 1] = 0\n",
    "test_set[test_set == 2] = 0\n",
    "test_set[test_set >= 3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kkL8NkkdlZj"
   },
   "source": [
    "## Creating the architecture of the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Hm6cNsd2Wfwh"
   },
   "outputs": [],
   "source": [
    "class RBM():\n",
    "\n",
    "  def __init__(self, nv, nh):\n",
    "    '''initializing number_of_visible_node(nv), number_of_hidden_node(nh), weights and bias'''\n",
    "    self.W = torch.randn(nh, nv)  # creates random tensors\n",
    "    self.a = torch.randn(1, nh) # pytorch dont accept 1d tensor so we need to give batch and number of bias of hidden note\n",
    "    self.b = torch.randn(1, nv) # bias for visible node\n",
    "\n",
    "\n",
    "\n",
    "  def sample_h(self, x):\n",
    "    '''sample_h function computes the probabilities of the hidden units being active given the visible units, and then stochastically samples the\n",
    "    states of the hidden units based on these probabilities.'''\n",
    "\n",
    "    # finding probability of h given v (which is ~= sigmoid function)\n",
    "    # .mm is for tensor matrix multiplication. It calculates the probability of hidden neurons given visible neurons.\n",
    "    # x is the visible node, and self.W is the weight matrix.\n",
    "    # take transpose to match dimension, weight dimension are (hidden, input_nods) and dimension x will be (n, input_nodes) where n can 1 to any nnumber\n",
    "    wx = torch.mm(x, self.W.t())\n",
    "    # 'wx' represents the product of the visible layer values and the transposed weight matrix.\n",
    "    # It calculates the linear combination of the weights and visible units for each data point.\n",
    "\n",
    "\n",
    "    activation = wx + self.a.expand_as(wx)\n",
    "    # dimesion of batch might not match with wx and we need to make sure bias is added to eachline of the mini-batch so we used expand_as(wx) expand as much as weights\n",
    "    # 'self.a' is a bias term for the hidden layer. It is added to each row of 'wx'.\n",
    "\n",
    "    P_h_given_v = torch.sigmoid(activation)\n",
    "    # 'P_h_given_v' represents the probability of the hidden units being activated given the visible units.\n",
    "    # It's calculated using the sigmoid function, which maps the linear combination to values between 0 and 1.\n",
    "\n",
    "    return P_h_given_v, torch.bernoulli(P_h_given_v)\n",
    "    # The function returns two values:\n",
    "    # 1. 'P_h_given_v': The probabilities of the hidden units being activated given the visible units.\n",
    "    # 2. 'torch.bernoulli(P_h_given_v)': This is a sampling operation. It samples binary values (0 or 1) for the hidden units\n",
    "    #    based on the calculated probabilities. This step introduces stochasticity into the model.(calculates which neurons were activated based on P_h_given_v 1-activated)\n",
    "\n",
    "\n",
    "\n",
    "  def sample_v(self, y):\n",
    "    '''sample_v function computes the probabilities of the visible units being active given the hidden units, and then stochastically samples the\n",
    "    states of the visible units based on these probabilities.'''\n",
    "\n",
    "    wy = torch.mm(y, self.W)\n",
    "    # here y represent hidden node\n",
    "    # we dont need to take tranpose of weights as dimeention are (hidden, input) and dimension of y is (n, hidden)\n",
    "\n",
    "    activation = wy + self.b.expand_as(wy)\n",
    "\n",
    "    P_v_given_h = torch.sigmoid(activation)\n",
    "\n",
    "    return P_v_given_h, torch.bernoulli(P_v_given_h)\n",
    "\n",
    "\n",
    "  def train(self, v0, vk, ph0, phk):\n",
    "    '''Train RBM model using Contrastive Divergence (CD) which uses Gibbs Sampling.'''\n",
    "\n",
    "    # v0: input vactor containing review of all movies by one user\n",
    "    # vk: visible nodes obtained after k-samplings (steps involed are forward_pass[sample_h] and backward_pass[sample_v])\n",
    "    # ph0: vactor of probability that at first iteration the hidden node equal to 1 given value of v0\n",
    "    # phk: probabilities of hidden node after k-samplling given the value of vk\n",
    "\n",
    "    self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n",
    "    self.b += torch.sum((v0 - vk), 0)\n",
    "    # Sum along the 0th dimension (reducing the dimension)\n",
    "    self.a += torch.sum((ph0 - phk), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "k0v7LJTXTtnN"
   },
   "outputs": [],
   "source": [
    "nv = len(training_set[0]) # number of visible node\n",
    "nh = 100 # number of hidden node\n",
    "batch_size = 100\n",
    "\n",
    "# creating class instance\n",
    "rbm = RBM(nv,nh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gy59alAdloL"
   },
   "source": [
    "## Training the RBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "brnHQ9LnWgIn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1loss: tensor(0.3515)\n",
      "epoch: 2loss: tensor(0.2294)\n",
      "epoch: 3loss: tensor(0.2518)\n",
      "epoch: 4loss: tensor(0.2488)\n",
      "epoch: 5loss: tensor(0.2468)\n",
      "epoch: 6loss: tensor(0.2494)\n",
      "epoch: 7loss: tensor(0.2488)\n",
      "epoch: 8loss: tensor(0.2453)\n",
      "epoch: 9loss: tensor(0.2464)\n",
      "epoch: 10loss: tensor(0.2488)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 10\n",
    "\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "\n",
    "  train_loss = 0 # simple difference in abs vaalue is the loss function in out model\n",
    "  s = 0.0 # counter to normalize train loss, by dividing loss by counter\n",
    "\n",
    "  for id_user in range(0, nb_users - batch_size, batch_size):\n",
    "\n",
    "    vk = training_set[id_user : id_user + batch_size]\n",
    "    # vk will be output of gibs sampling[id_user + 100]\n",
    "\n",
    "    v0 = training_set[id_user : id_user + batch_size]\n",
    "    # original ratings\n",
    "\n",
    "    ph0,_ = rbm.sample_h(v0)\n",
    "    # probability of hidden node being 1 at first iteration given input node v0\n",
    "    # sample_h returns two value and we only need probability which first value that is why we user '_'\n",
    "\n",
    "    # for loop for k steps for Contrastive Divergence\n",
    "    for k in range(10):\n",
    "\n",
    "      '''at the end we get 10th sample of hidden and visible node'''\n",
    "\n",
    "      _,hk = rbm.sample_h(vk)\n",
    "      # hidden nodes obtained at  k steps for Contrastive Divergence\n",
    "\n",
    "      _,vk = rbm.sample_v(hk)\n",
    "      # visible nodes after sampling\n",
    "\n",
    "      vk[v0<0] = v0[v0<0]\n",
    "      # we need to keep the node which were not rated to same -1 value\n",
    "\n",
    "    phk,_ = rbm.sample_h(vk)\n",
    "    #  probabilities of hidden node after k-samplling given the value of vk\n",
    "\n",
    "    rbm.train(v0, vk, ph0, phk)\n",
    "    train_loss += torch.mean(torch.abs(v0[v0>=0] - vk[v0>=0])) # we only want to calculate loss for rating that exist\n",
    "    s += 1.0\n",
    "\n",
    "  print('epoch: ' + str(epoch) + 'loss: ' + str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bak5uc8gd-gX"
   },
   "source": [
    "## Testing the RBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "a4DIuCcGWgdj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(0.2341)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_loss = 0 # simple difference in abs vaalue is the loss function in out model\n",
    "s = 0.0 # counter to normalize test loss, by dividing loss by counter\n",
    "\n",
    "for id_user in range(nb_users):\n",
    "\n",
    "  v = training_set[id_user : id_user + 1]\n",
    "  # v will be output of gibs sampling which we will compare to the output of test_set\n",
    "  # we are using training_set to activate neuron to predict test_set\n",
    "\n",
    "  vt = test_set[id_user : id_user + 1]\n",
    "  # original ratings\n",
    "\n",
    "  # ph0 is not needed to train data and we dont need to re-calculate for test\n",
    "  # probability of hidden node being 1 at first iteration given input node v0\n",
    "\n",
    "  if len(vt[vt>=0]) > 0:\n",
    "\n",
    "    _,h = rbm.sample_h(v)\n",
    "    # hidden nodes obtained at  k steps for Contrastive Divergence\n",
    "\n",
    "    _,v = rbm.sample_v(h)\n",
    "    # visible nodes after sampling\n",
    "\n",
    "    test_loss += torch.mean(torch.abs(vt[vt>=0] - v[vt>=0]))\n",
    "    s += 1.0\n",
    "\n",
    "print('test loss: ' + str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''With this metric, we obtained an Average Distance of 0.24, which is equivalent to about 75% of correct prediction.\n",
    "\n",
    "Hence, it works very well and there is a predictive power.'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
